

<!doctype html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>understanding_logistic_regression module &mdash; sideprojects  documentation</title>
    
    <link rel="stylesheet" href="/~prastog3/_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="/~prastog3/_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="/~prastog3/_static/jquery.js"></script>
    <script type="text/javascript" src="/~prastog3/_static/underscore.js"></script>
    <script type="text/javascript" src="/~prastog3/_static/doctools.js"></script>
    <script type="text/javascript" src="/~prastog3/_static/bizstyle.js"></script>
    <link rel="top" title="sideprojects  documentation" href="index.html" />
    <link rel="prev" title="sample_complexities module" href="sample_complexities.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="/~prastog3/_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="sample_complexities.html" title="sample_complexities module"
             accesskey="P">previous</a></li>
        <li class="nav-item nav-item-0"><a href="index.html">sideprojects  documentation</a> &raquo;</li> 
      </ul>
    </div>

    <div class="document">
      <div class="documentwrapper">
          <div class="body" role="main">
            
  <div class="section" id="module-understanding_logistic_regression">
<span id="understanding-logistic-regression-module"></span><h1>understanding_logistic_regression module<a class="headerlink" href="#module-understanding_logistic_regression" title="Permalink to this headline">¶</a></h1>
<div class="section" id="filename-understanding-logistic-regression-py">
<h2>| Filename: understanding_logistic_regression.py<a class="headerlink" href="#filename-understanding-logistic-regression-py" title="Permalink to this headline">¶</a></h2>
<div class="line-block">
<div class="line">Description: This script demonstrates the following:</div>
<div class="line-block">
<div class="line">1. The SGDClassifier class in sklearn</div>
<div class="line">2. The use of progressive validation</div>
<div class="line">3. The fact that accurate recovery of true parameters of the</div>
<div class="line-block">
<div class="line">statistical model that generated data is not necessary for good</div>
<div class="line">predictive performance.</div>
</div>
<div class="line">4. The fact that conditional loglikelihood maximization (as opposed</div>
<div class="line-block">
<div class="line">to joint MLE) can still recover the true parameters of the</div>
<div class="line">conditional distribution that generated the data.</div>
</div>
</div>
<div class="line">Author: Pushpendre Rastogi</div>
<div class="line">Created: Wed Jul 22 17:55:17 2015 (-0400)</div>
<div class="line">Last-Updated:</div>
<div class="line-block">
<div class="line-block">
<div class="line">By:</div>
</div>
<div class="line">Update #: 248</div>
</div>
</div>
<p>The choice of Loglinear distribution can be justified through the fact
that the log linear distribution
achieves maximum entropy subject to constraints over means of the
feature values. It turns out that doing so amounts to MLE over the
conditional loglikelihood (so not really MLE but close enough).</p>
<p>In this script at each iteration we would sample the LogLinearBinaryRV.
And by finding the parameters that maximize the regularized conditional
log-likelihood through stochastic gradient descent we would try to
recover the true parameters of the probability distribution that
generate the data.</p>
<p>Since this is a problem of recovering the parameters of a parametric model
This is definitely a statistical problem. Note that in the sense
that a statistical method is a correct algorithm for figuring out
the true parameters for a distribution from the data a statistician
is a computer scientist creating an algorithm.</p>
<p>For example the fact that MLE is a consistent estimator is a proof
that an algorithm that solves the MLE problem is the correct
algorithm for figuring out the parameters of a distribution.</p>
<p>Aside from these stated goals we also answer the following questions.</p>
<ol class="arabic simple">
<li>Was training even necessary?
As in could I have attained the same level performance even without
training the SGD classifier. This is answered by just finding the
performance of the untrained classifier.</li>
<li>Can I improve the performance further?
One way is to find out the performance of the bayes optimal
predictor based on the true generating distribution. If the performance
of the fitted method on the holdout set is close to the bayes optimal
predictor then we know that we have done the best we could.</li>
<li>How sensitive is the performance of the ideal bayes optimal predictor to the
distribution of the true data?
(Very sensitive, most importantly it is sensitive to the bandwidth)</li>
<li>How sensitive is it to changes in magnitude?</li>
<li>What is the performance of the naive bayes estimator?
Well Jordan/Ng already gave theorems about the performance of the
generative-discriminative pairs.</li>
</ol>
</div>
<div class="section" id="note-progressive-validation">
<h2>NOTE: Progressive validation<a class="headerlink" href="#note-progressive-validation" title="Permalink to this headline">¶</a></h2>
<p>PV is a method for
estimating the test set performance of a hypothesis learnt by a
learning algorithm using just the training set.
This technique was originally presented by John Langford in a very
different way in his thesis. But one sentence from his thesis works well:
&#8220;The progressive validation protocol has a learner repeatedly commit
to a hypothesis before it is given a new example.&#8221;
This is exactly what we are doing with sgd based learning. At every
point during training the learner has formed a hypothesis based on
the examples that were presented to it so far and we ask the learner
to predict the outcome for an example before using that example for
training and we keep an average of the average loss incurred by our
learner during the entire training process.
We can compare it to the standard cross-validation based estimates
of performance and holdout based estimates of performance.</p>
<hr class="docutils" />
<dl class="class">
<dt id="understanding_logistic_regression.LogLinearBinaryRV">
<em class="property">class </em><code class="descclassname">understanding_logistic_regression.</code><code class="descname">LogLinearBinaryRV</code><span class="sig-paren">(</span><em>coeff</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#LogLinearBinaryRV"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.LogLinearBinaryRV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>This class represents a loglinear conditional distribution
p(y | X) that can be initialized with certain coefficients
and which can be sampled and it can tell us the probability of
the output.</p>
<dl class="method">
<dt id="understanding_logistic_regression.LogLinearBinaryRV.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#LogLinearBinaryRV.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.LogLinearBinaryRV.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="understanding_logistic_regression.LogLinearBinaryRV.probability">
<code class="descname">probability</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#LogLinearBinaryRV.probability"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.LogLinearBinaryRV.probability" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the true probability of class 1 given the input</p>
</dd></dl>

<dl class="method">
<dt id="understanding_logistic_regression.LogLinearBinaryRV.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#LogLinearBinaryRV.sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.LogLinearBinaryRV.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="understanding_logistic_regression.PngPlotter">
<em class="property">class </em><code class="descclassname">understanding_logistic_regression.</code><code class="descname">PngPlotter</code><span class="sig-paren">(</span><em>X=None</em>, <em>Y=None</em>, <em>marker='o'</em>, <em>title=''</em>, <em>xlabel=''</em>, <em>ylabel=''</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#PngPlotter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.PngPlotter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="understanding_logistic_regression.PngPlotter.add_data">
<code class="descname">add_data</code><span class="sig-paren">(</span><em>x</em>, <em>y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#PngPlotter.add_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.PngPlotter.add_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="understanding_logistic_regression.PngPlotter.render">
<code class="descname">render</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#PngPlotter.render"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.PngPlotter.render" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="understanding_logistic_regression.log_loss">
<code class="descclassname">understanding_logistic_regression.</code><code class="descname">log_loss</code><span class="sig-paren">(</span><em>y</em>, <em>pyeq1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#log_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.log_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="understanding_logistic_regression.loglinear_prob">
<code class="descclassname">understanding_logistic_regression.</code><code class="descname">loglinear_prob</code><span class="sig-paren">(</span><em>X</em>, <em>coeff</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#loglinear_prob"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.loglinear_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>The probability p(y = 1 | X) and coeff values</p>
</dd></dl>

<dl class="function">
<dt id="understanding_logistic_regression.orthogonal_portion">
<code class="descclassname">understanding_logistic_regression.</code><code class="descname">orthogonal_portion</code><span class="sig-paren">(</span><em>v</em>, <em>e</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/understanding_logistic_regression.html#orthogonal_portion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#understanding_logistic_regression.orthogonal_portion" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the portion of v that is not along e.</p>
</dd></dl>

</div>
<div class="highlight-python"><div class="highlight"><pre><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">================================================</span>
<span class="sd">| Filename: understanding_logistic_regression.py</span>
<span class="sd">================================================</span>

<span class="sd">| Description: This script demonstrates the following:</span>
<span class="sd">|  1. The SGDClassifier class in sklearn</span>
<span class="sd">|  2. The use of progressive validation</span>
<span class="sd">|  3. The fact that accurate recovery of true parameters of the</span>
<span class="sd">|     statistical model that generated data is not necessary for good</span>
<span class="sd">|     predictive performance.</span>
<span class="sd">|  4. The fact that conditional loglikelihood maximization (as opposed</span>
<span class="sd">|     to joint MLE) can still recover the true parameters of the</span>
<span class="sd">|     conditional distribution that generated the data.</span>
<span class="sd">| Author: Pushpendre Rastogi</span>
<span class="sd">| Created: Wed Jul 22 17:55:17 2015 (-0400)</span>
<span class="sd">| Last-Updated:</span>
<span class="sd">|           By:</span>
<span class="sd">|     Update #: 248</span>

<span class="sd">The choice of Loglinear distribution can be justified through the fact</span>
<span class="sd">that the log linear distribution</span>
<span class="sd">achieves maximum entropy subject to constraints over means of the</span>
<span class="sd">feature values. It turns out that doing so amounts to MLE over the</span>
<span class="sd">conditional loglikelihood (so not really MLE but close enough).</span>

<span class="sd">In this script at each iteration we would sample the LogLinearBinaryRV.</span>
<span class="sd">And by finding the parameters that maximize the regularized conditional</span>
<span class="sd">log-likelihood through stochastic gradient descent we would try to</span>
<span class="sd">recover the true parameters of the probability distribution that</span>
<span class="sd">generate the data.</span>

<span class="sd">Since this is a problem of recovering the parameters of a parametric model</span>
<span class="sd">This is definitely a statistical problem. Note that in the sense</span>
<span class="sd">that a statistical method is a correct algorithm for figuring out</span>
<span class="sd">the true parameters for a distribution from the data a statistician</span>
<span class="sd">is a computer scientist creating an algorithm.</span>

<span class="sd">For example the fact that MLE is a consistent estimator is a proof</span>
<span class="sd">that an algorithm that solves the MLE problem is the correct</span>
<span class="sd">algorithm for figuring out the parameters of a distribution.</span>


<span class="sd">Aside from these stated goals we also answer the following questions.</span>

<span class="sd">1. Was training even necessary?</span>
<span class="sd">   As in could I have attained the same level performance even without</span>
<span class="sd">   training the SGD classifier. This is answered by just finding the</span>
<span class="sd">   performance of the untrained classifier.</span>
<span class="sd">2. Can I improve the performance further?</span>
<span class="sd">   One way is to find out the performance of the bayes optimal</span>
<span class="sd">   predictor based on the true generating distribution. If the performance</span>
<span class="sd">   of the fitted method on the holdout set is close to the bayes optimal</span>
<span class="sd">   predictor then we know that we have done the best we could.</span>
<span class="sd">3. How sensitive is the performance of the ideal bayes optimal predictor to the</span>
<span class="sd">   distribution of the true data?</span>
<span class="sd">   (Very sensitive, most importantly it is sensitive to the bandwidth)</span>
<span class="sd">4. How sensitive is it to changes in magnitude?</span>
<span class="sd">5. What is the performance of the naive bayes estimator?</span>
<span class="sd">   Well Jordan/Ng already gave theorems about the performance of the</span>
<span class="sd">   generative-discriminative pairs.</span>

<span class="sd">============================</span>
<span class="sd">NOTE: Progressive validation</span>
<span class="sd">============================</span>

<span class="sd">PV is a method for</span>
<span class="sd">estimating the test set performance of a hypothesis learnt by a</span>
<span class="sd">learning algorithm using just the training set.</span>
<span class="sd">This technique was originally presented by John Langford in a very</span>
<span class="sd">different way in his thesis. But one sentence from his thesis works well:</span>
<span class="sd">&quot;The progressive validation protocol has a learner repeatedly commit</span>
<span class="sd">to a hypothesis before it is given a new example.&quot;</span>
<span class="sd">This is exactly what we are doing with sgd based learning. At every</span>
<span class="sd">point during training the learner has formed a hypothesis based on</span>
<span class="sd">the examples that were presented to it so far and we ask the learner</span>
<span class="sd">to predict the outcome for an example before using that example for</span>
<span class="sd">training and we keep an average of the average loss incurred by our</span>
<span class="sd">learner during the entire training process.</span>
<span class="sd">We can compare it to the standard cross-validation based estimates</span>
<span class="sd">of performance and holdout based estimates of performance.</span>

<span class="sd">------------------------------------------------------------------</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">with_statement</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">typecheck</span> <span class="kn">import</span> <span class="n">typecheck</span>


<span class="k">def</span> <span class="nf">loglinear_prob</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">coeff</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;The probability p(y = 1 | X) and coeff values</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">coeff</span><span class="p">)))</span>


<span class="k">class</span> <span class="nc">LogLinearBinaryRV</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39; This class represents a loglinear conditional distribution</span>
<span class="sd">    p(y | X) that can be initialized with certain coefficients</span>
<span class="sd">    and which can be sampled and it can tell us the probability of</span>
<span class="sd">    the output.</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coeff</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="o">=</span> <span class="n">coeff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">independent_var</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">independent_var</span><span class="p">,))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">noise_level</span>
        <span class="n">coeff_alignment_sign</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">coeff_alignment</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="o">*</span> \
            <span class="n">args</span><span class="o">.</span><span class="n">coeff_strength</span> <span class="o">*</span> <span class="n">coeff_alignment_sign</span>
        <span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">noise</span> <span class="o">+</span> <span class="n">coeff_alignment</span><span class="p">)</span>
        <span class="c"># Having two classes of coefficients is like having double the</span>
        <span class="c"># number of features.</span>
        <span class="n">positive_class_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">probability</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span>
                  <span class="k">if</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">positive_class_prob</span>
                  <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; Return the true probability of class 1 given the input &#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="n">loglinear_prob</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">probability</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="mi">0</span>


<span class="k">class</span> <span class="nc">PngPlotter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">X</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">Y</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span><span class="p">,</span>
            <span class="n">title</span><span class="o">=</span><span class="s">&#39;&#39;</span><span class="p">,</span>
            <span class="n">xlabel</span><span class="o">=</span><span class="s">&#39;&#39;</span><span class="p">,</span>
            <span class="n">ylabel</span><span class="o">=</span><span class="s">&#39;&#39;</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; X and Y are lists of X and Y values of points &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">X</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">Y</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">Y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">marker</span> <span class="o">=</span> <span class="n">marker</span>

    <span class="k">def</span> <span class="nf">add_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>


<span class="k">def</span> <span class="nf">log_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pyeq1</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pyeq1</span>
                       <span class="k">if</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">1</span>
                       <span class="k">else</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">pyeq1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">orthogonal_portion</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">e</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39; Return the portion of v that is not along e.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">e</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">v</span> <span class="o">-</span> <span class="n">e</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>


<span class="nd">@typecheck</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">,</span> <span class="n">LogLinearBinaryRV</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">get_holdout_accuracy</span><span class="p">(</span><span class="n">holdout_set</span><span class="p">,</span> <span class="n">predictor_obj</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">predictor_obj</span><span class="p">,</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">):</span>
        <span class="n">prediction_fnc</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">X</span><span class="p">:</span> <span class="n">predictor_obj</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">predictor_obj</span><span class="p">,</span> <span class="n">LogLinearBinaryRV</span><span class="p">):</span>
        <span class="n">prediction_fnc</span> <span class="o">=</span> <span class="n">predictor_obj</span><span class="o">.</span><span class="n">predict</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">y</span> <span class="o">==</span> <span class="n">prediction_fnc</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="ow">in</span> <span class="n">holdout_set</span><span class="p">])</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">arg_parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s">&#39;Understanding Logistic Regression&#39;</span><span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&#39;--loss&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">&#39;log&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                            <span class="n">help</span><span class="o">=</span><span class="s">&#39;Objective that is optimized, [log|perceptron|hinge]&#39;</span><span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&#39;--penalty&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">&#39;l2&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                            <span class="n">help</span><span class="o">=</span><span class="s">&#39;Penalty type, [l1|l2|elasticnet]&#39;</span><span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&#39;--alpha&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                            <span class="n">help</span><span class="o">=</span><span class="s">&#39;The weight of the regularizer&#39;</span><span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&#39;--train_sample_count&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s">&#39;--validation_sample_count&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&#39;--coeff_strength&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                            <span class="n">help</span><span class="o">=</span><span class="s">&#39;How close to true coeff is p(X)?&#39;</span><span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&#39;--noise_level&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                            <span class="n">help</span><span class="o">=</span><span class="s">&#39;The strength of noise to add.&#39;</span><span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s">&#39;--bandwidth&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">&#39;Default={1.0}&#39;</span><span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s">&#39;--prob_dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">&#39;Default={100}&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">arg_parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="c"># The true coefficients that define the log-linear distribution p(y|x)</span>
    <span class="n">coeff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">(</span>
        <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">prob_dim</span><span class="p">,))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">prob_dim</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">coeff</span> <span class="o">*=</span> <span class="n">args</span><span class="o">.</span><span class="n">bandwidth</span>
    <span class="n">LogLinearConditionalPDFObject</span> <span class="o">=</span> <span class="n">LogLinearBinaryRV</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
    <span class="n">clsfr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>        <span class="c"># log, perceptron, hinge</span>
        <span class="n">penalty</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>  <span class="c"># l1, l2, elasticnet</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>      <span class="c"># Alpha is the weight of the regularizer</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>   <span class="c"># Fit the bias/intercept term or not</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>      <span class="c"># When set to True, reuse the solution of the previous call</span>
    <span class="n">prog_valid_plotter</span> <span class="o">=</span> <span class="n">PngPlotter</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">&#39;Progressive Validation&#39;</span><span class="p">)</span>
    <span class="n">coeff_error_plotter</span> <span class="o">=</span> <span class="n">PngPlotter</span><span class="p">(</span>
        <span class="n">title</span><span class="o">=</span><span class="s">&#39;Relative Squared Error of ditribution parameters&#39;</span><span class="p">)</span>
    <span class="n">prog_loss_plotter</span> <span class="o">=</span> <span class="n">PngPlotter</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">&#39;Progressive Log Loss&#39;</span><span class="p">)</span>
    <span class="n">prog_valid</span><span class="p">,</span> <span class="n">coeff_error</span><span class="p">,</span> <span class="n">prog_regret_log_loss</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>
    <span class="c"># Create a holdout set based on samples from the</span>
    <span class="n">holdout_set</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">LogLinearConditionalPDFObject</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
            <span class="n">args</span><span class="o">.</span><span class="n">validation_sample_count</span><span class="p">)]</span>

    <span class="c"># Find the holdout accuracy of the bayes optimal classifier</span>
    <span class="n">holdout_accuracy_bayes_optimal</span> <span class="o">=</span> <span class="n">get_holdout_accuracy</span><span class="p">(</span>
        <span class="n">holdout_set</span><span class="p">,</span> <span class="n">LogLinearConditionalPDFObject</span><span class="p">)</span>
    <span class="c"># Performance of the majority baseline</span>
    <span class="n">mbp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">y</span> <span class="k">for</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="ow">in</span> <span class="n">holdout_set</span><span class="p">])</span>
    <span class="n">mbp</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">mbp</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">mbp</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\n</span><span class="s"> mbp&#39;</span><span class="p">,</span> <span class="n">mbp</span><span class="p">,</span> <span class="p">)</span>
    <span class="c"># Fit once before starting progressive validation.</span>
    <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="n">LogLinearConditionalPDFObject</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">clsfr</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">y</span><span class="p">],</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">clsfr</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">clsfr</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c"># Holdout accuracy before starting training</span>
    <span class="n">holdout_accuracy_before_training</span> <span class="o">=</span> <span class="n">get_holdout_accuracy</span><span class="p">(</span><span class="n">holdout_set</span><span class="p">,</span> <span class="n">clsfr</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_sample_count</span><span class="p">):</span>
        <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="n">LogLinearConditionalPDFObject</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">clsfr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prog_valid</span> <span class="o">*=</span> <span class="nb">float</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">prog_valid</span> <span class="o">+=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">y_hat</span> <span class="o">==</span> <span class="n">y</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">coeff_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>
            <span class="n">clsfr</span><span class="o">.</span><span class="n">coef_</span> <span class="o">-</span> <span class="n">coeff</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
        <span class="n">true_log_prob</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="n">LogLinearConditionalPDFObject</span><span class="o">.</span><span class="n">probability</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">estimated_log_prob</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="n">loglinear_prob</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="n">clsfr</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
        <span class="n">prog_regret_log_loss</span> <span class="o">*=</span> <span class="nb">float</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">prog_regret_log_loss</span> <span class="o">+=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">true_log_prob</span> <span class="o">-</span>
                                          <span class="n">estimated_log_prob</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">prog_valid_plotter</span><span class="o">.</span><span class="n">add_data</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">prog_valid</span><span class="p">)</span>
        <span class="n">coeff_error_plotter</span><span class="o">.</span><span class="n">add_data</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">coeff_error</span><span class="p">)</span>
        <span class="n">prog_loss_plotter</span><span class="o">.</span><span class="n">add_data</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">prog_regret_log_loss</span><span class="p">)</span>
        <span class="n">clsfr</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">y</span><span class="p">],</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="c"># Now test the performance of the fitted logistic classifier</span>
    <span class="n">holdout_accuracy_after_training</span> <span class="o">=</span> <span class="n">get_holdout_accuracy</span><span class="p">(</span><span class="n">holdout_set</span><span class="p">,</span> <span class="n">clsfr</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span>
        <span class="s">&#39;</span><span class="se">\n</span><span class="s"> prog_valid&#39;</span><span class="p">,</span> <span class="n">prog_valid</span><span class="p">,</span>
        <span class="s">&#39;</span><span class="se">\n</span><span class="s"> holdout_accuracy_bayes_optimal&#39;</span><span class="p">,</span> <span class="n">holdout_accuracy_bayes_optimal</span><span class="p">,</span>
        <span class="s">&#39;</span><span class="se">\n</span><span class="s"> holdout_accuracy_before_training&#39;</span><span class="p">,</span> <span class="n">holdout_accuracy_before_training</span><span class="p">,</span>
        <span class="s">&#39;</span><span class="se">\n</span><span class="s"> holdout_accuracy_after_training&#39;</span><span class="p">,</span> <span class="n">holdout_accuracy_after_training</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c"># Find out the recovered parameters versus the true parameters</span>
    <span class="c"># NOTE: clsfr.coef_ is the recovered parameter and coeff was the</span>
    <span class="c"># true parameter</span>
    <span class="n">erroneous_vec_fraction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>
        <span class="n">orthogonal_portion</span><span class="p">(</span><span class="n">clsfr</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">coeff</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">clsfr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span>
        <span class="s">&#39;</span><span class="se">\n</span><span class="s"> erroneous_vec_fraction&#39;</span><span class="p">,</span> <span class="n">erroneous_vec_fraction</span><span class="p">,</span>
        <span class="s">&#39;</span><span class="se">\n</span><span class="s"> prog_regret_log_loss&#39;</span><span class="p">,</span> <span class="n">prog_regret_log_loss</span>
    <span class="p">)</span>
    <span class="n">prog_valid_plotter</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
    <span class="n">coeff_error_plotter</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
    <span class="n">prog_loss_plotter</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
    <span class="c"># Log Loss of the optimal predictor.</span>
    <span class="n">log_loss_optimal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="p">[</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">LogLinearConditionalPDFObject</span><span class="o">.</span><span class="n">probability</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
         <span class="k">for</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
         <span class="ow">in</span> <span class="n">holdout_set</span><span class="p">])</span>
    <span class="n">log_loss_estimated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="p">[</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loglinear_prob</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">clsfr</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
         <span class="k">for</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
         <span class="ow">in</span> <span class="n">holdout_set</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\n</span><span class="s"> log_loss_optimal&#39;</span><span class="p">,</span> <span class="n">log_loss_optimal</span><span class="p">,</span>
          <span class="s">&#39;</span><span class="se">\n</span><span class="s"> log_loss_estimated&#39;</span><span class="p">,</span> <span class="n">log_loss_estimated</span><span class="p">,</span> <span class="p">)</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Look at how the log-loss truy is minimized but the validation error</span>
<span class="sd">    is basically the same even when we use the hinge loss.</span>
<span class="sd">    log 0.885 0.237379757873 0.0708640472617</span>
<span class="sd">    log 0.894 0.128038074657 0.0550941428819</span>
<span class="sd">    log 0.874 0.133218984176 0.0914931968861</span>
<span class="sd">    hinge 0.895 0.473717543562 0.215030860105</span>
<span class="sd">    hinge 0.89 0.434286919844 0.216065130295</span>
<span class="sd">    hinge 0.87 0.462364313401 0.240906415183</span>
<span class="sd">    perceptron 0.81 1.00405480928 0.619301226865</span>
<span class="sd">    perceptron 0.767 1.00245422266 0.6231903911</span>
<span class="sd">    perceptron 0.819 0.999211001065 0.601749697036</span>

<span class="sd">    Note that I am only able to recover the</span>
<span class="sd">    python understanding_logistic_regression.py --alpha 0.01 --coeff_strength 0.0 --train_sample_count 80000</span>
<span class="sd">    0.519575 0.684376979344 0.0391865540893</span>
<span class="sd">    0.5225 0.519575</span>
<span class="sd">    0.692045918504 0.692365926667</span>
<span class="sd">    python understanding_logistic_regression.py --alpha 0.01 --coeff_strength 1.0 --train_sample_count 80000</span>
<span class="sd">    0.7309375 0.0683319657749 0.0253992530515</span>
<span class="sd">    0.7264 0.7309375</span>
<span class="sd">    0.585110828186 0.585081212155</span>
<span class="sd">    python understanding_logistic_regression.py --alpha 0.01 --coeff_strength 8.0 --train_sample_count 80000</span>
<span class="sd">    0.9997 0.151844644062 0.000821186056188</span>
<span class="sd">    0.9994 0.9997</span>
<span class="sd">    0.00516971678716 0.00518374450427</span>
<span class="sd">    Really what this tells me is that there are limits to recovering the</span>
<span class="sd">    true parameters of the true system and if we only try to build a</span>
<span class="sd">    good predictor we can often more succeed more spectacularly than if</span>
<span class="sd">    we try to estimate the parameters of the system that generated the</span>
<span class="sd">    data.</span>

<span class="sd">    mbp 0.5174</span>

<span class="sd">    prog_valid 0.727</span>
<span class="sd">    holdout_accuracy_bayes_optimal 0.7546</span>
<span class="sd">    holdout_accuracy_before_training 0.5174</span>
<span class="sd">    holdout_accuracy_after_training 0.7432</span>

<span class="sd">    erroneous_vec_fraction 0.546445772363</span>
<span class="sd">    prog_regret_log_loss 0.216014496595</span>

<span class="sd">    log_loss_optimal 0.530312347206</span>
<span class="sd">    log_loss_estimated 0.548764473949</span>
<span class="sd">    &#39;&#39;&#39;</span>
</pre></div>
</div>
</div>


          </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="sample_complexities.html" title="sample_complexities module"
             >previous</a></li>
        <li class="nav-item nav-item-0"><a href="index.html">sideprojects  documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2015, Author.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>
