---
title:  "Multiview LSA Proofs and FAQ"
layout: post
date:   2015-06-09 17:41:10
---
<!-- <div class="definition"> </div> -->
When I was presenting [my paper about MVLSA](mvlsa/mvlsa.pdf) at NAACL
I felt that I had
left out all the interesting background and motivation behind
GCCA from the paper and only gave the most essential pieces of the
algorithm.
I still think that it was a good call
since the main point of the paper was to evaluate GCCA's
performance and my own contribution was just to make sure that its
computation was fast. It's not like I derived GCCA itself.

Anyway, a few people expressed interested in the
derivation/motivation behind the method so I will write it down here.
In this article I am going to start from scratch, first present (in
words) the motivation that led to GCCA  and then show how equation 1
and equation 3 result from it.

Once again, I'll note that I am merely rephrasing something that was already derived by
{% cite carroll1968generalization %} and
{% cite kettenring1971canonical %}. Though I do think that
Kettenring's notation is too old and readers might prefer the
simpler notation that I'll use. Of course the blame for mistakes rests
with me. Let me know if you find any:

**What is the motivation/backstory behind criteria 1 in the paper ?**

Alternatively, the question is that why do I say that the
objective of MAXVAR-GCCA is:

$$\arg \min_{G, U_j} \sum_{j=1}^J  || G - X_j U_j ||^2_F \text{ such that } G^TG = I $$

Just like PCA has at least 3 different interpretations/derivations
GCCA too has multiple interpretations. Let me first state the
motivations in words:

1. MAXVAR GCCA finds a representation that maximizes
   total squared correlation between an auxilliary variable
   and possible rank-$$r$$ linear transformations/distortions of
   the data. (This is *Carroll's starting point*)

2. MAXVAR GCCA of rank $$r$$ finds a representation $$G$$
   that minimizes the total squared distance from rank-$$r$$
   subspace spanned by the data points available in different
   views. (This corresponds to *criteria 1*. {% cite sengupta1983generalized %} reports that
    it was {% cite coelho1992generalized %}
    who first gave this "geometric" interpretation corresponding to the
    traditional statistical one.)){% fn  %}

3. There are two closely related versions, that talk about :

   a. MAXVAR GCCA finds projections of the data that make the
      inter projection correlation matrix as close to a rank 1 matrix
      as possible (This was how {% cite horst1961generalized %}
      derived MAXVAR GCCA )

   b. MAXVAR GCCA finds projections of the data that maximize the
      highest eigen value of the inter projection correlation matrix.
      (This was the Interpretation that
      {% cite kettenring1971canonical %} gave. In his paper he basically
      unified 4-5 different types of GCCA as optimizations of different measures
      of the inter-correlation matrix. This was why he worded
      rephrased Horst's criteria like this)

Now the trouble is that even though the above statements
are small they are woefully imprecise. What is "inter projection
correlation matrix"? What is "total squared distance"? What does it
mean that a matrix is "as close to a rank 1 matrix" as possible?
{% fn %} To really understand how all these criteria can have the same
solution we need to use precise mathematical notation.

*Let us look at a simplified version of Carroll's original
derivation.*

Imagine that we have multiple sources of vector valued data
$$ x_j $$ all of which have zero mean.
Smallcase letters are random variables.
Letters in bold are vectors or matrices.
Letters in romanscript are some fixed unknown variables, i.e. they are
not random variables.
Let's say we want to find a single scalar linear projection
$$ z_j $$ of each data source
that is $$ z_j = x_j \cdot \mathrm{u}_j $$ for some $$ \mathrm{u}_j $$ such that $$ z_j $$
and some auxilliary random variable $$ g $$ have as high total correlation as
possible. Essentially this criteria is saying that there are
linear components in the different
sources of data that I observed that were generated by a noisy linear transformation of
of a single underlying random variable.

$$ (z_j = x_j \cdot \mathrm{u}_j) = \mathrm{a}_j g + \epsilon $$

Note that essentially we are doing factor analysis on $$ z_j $$ which
are linearly derived from $$ x_j $$. If we would have done PCA by
straight up concatenating the sources of data then we would have been doing factor
analysis of the actual data instead of some projection of
it.{% fn %}
There is considerably less flexibility in PCA. Also,
the GCCA could recover PCA's output if we chose

$$ g $$ and $$ epsilon $$ are random variables that are sampled with
each observation.
Another constraint is that $$ z_j $$ should have
   unit variance, and because $$ x_j $$ is zero mean, therefore
   $$ Z_j^T Z_j = n $$ where $$ Z_j = X_j u_j $$
   and since we are only interested in the correlation of the projections
   therefore it makes sense to just constrain the variance of
   $$ g $$ to be equal to 1. {% fn %}
   I.e. we want to maximize

   $$ \begin{align*}
   & \arg \max \sum_{j=1}^J \text{corr}( z_j, g )^2 \\
   & \text{subject to } \text{var}(z_j) = 1, \text{var}(g) = 1
   \end{align*}
   $$

   Now because of all the contraints we can loose the correlation and
   talk about covariances instead, so the optimization problem becomes

   $$ \begin{align*}
   & \arg \max \sum_{j=1}^J \text{cov}( z_j, g )^2 \\
   & \text{subject to } \text{var}(z_j) = 1, \text{var}(g) = 1
   \end{align*}
   $$

   Now $$ x_j, z_j, g $$ are all mean centered so we can replace
   covariance by its sample estimate $$ \frac{Z_j^TG}{ n } $$ where $$ n
   $$ is the number of samples in data. Note that $$ Z_j, G $$
   contain actual data, they are not random variables and they are
   vectors for now but we will generalize later. so the problem becomes

   $$ \begin{align*}
   & \arg \max \sum_{j=1}^J ( \frac{Z_j^T G}{ n } )^2 \\
   & \text{subject to } Z_j^TZ_j = n, G^TG = n
   \end{align*}
   $$

   Now getting rid of the $$ n $$ is just a simple change of
   variable $$ G' = G/\sqrt{n} $$ and $$ Z_j' = Z_j/\sqrt{n} $$.
   Let's assume that we did that change but kept the old variables

   $$ \begin{align*}
   & \arg \max \sum_{j=1}^J ( {Z_j^T G} )^2 \\
   & \text{subject to } Z_j^TZ_j = 1, G^TG = 1
   \end{align*}
   $$

   Now for any fixed $$ G $$ that would be the maxima of this
   problem. The $$ Z_j $$ are exclusively determined through
   <!-- If we let $$ Z $$ be the $$ n \times J $$ dimensional matrix whose -->
   <!-- $$ J $$ columns are made up of $$ Z_j $$ then -->
   <!-- we can rewrite $$ \sum_{j=1}^J ( {Z_j^T G} )^2 $$ -->
   <!-- as $$ {\text{trace}( (Z^TG)^T (Z^TG) )} $$. Every -->
   <!-- time we do this symbol manipulation we jump up to a higher vector -->
   <!-- space. Also note that since -->



## Footnotes ##
{% footnotes %}
   {% fnbody %}
    This is like the
   statistical-geometric interpretations of PCA.
   {% endfnbody %}
   {% fnbody %}
   But the problem is also that if we give the rigorous definitions
   without a summary of what those definitions and derivations should
   be interpreted as, then it becomes hard to maintain interest. So
   it's a chicken and egg problem.
   {% endfnbody %}
   {% fnbody %}

   {% endfnbody %}
   {% fnbody %}
      If all we are worried about is the correlation then the
     magnitude of  <script type="math/tex">g</script> is just a useless parameter which we can fix to 1
     for convenience
   {% endfnbody %}
   {% fnbody %}
   PCA and Factor analysis are the same thing for me, at least in this article.
   {% endfnbody %}
{% endfootnotes %}

## Bibliography ##
{% bibliography --cited %}
